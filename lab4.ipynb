{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruiqi/anaconda3/envs/mase/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of JSC_1923 parameters: 3285\n",
      "Total number of JSC_Tiny parameters: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to debug\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"rs1923/mase_real/machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity, get_logger\n",
    "\n",
    "from chop.passes.graph.analysis import (\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    ")\n",
    "from chop.passes.graph import (\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "\n",
    "set_logging_verbosity(\"debug\")\n",
    "\n",
    "logger = get_logger(\"chop\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "batch_size = 512\n",
    "model_name = \"jsc-tiny\"\n",
    "dataset_name = \"jsc\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = {\"x\": next(iter(data_module.train_dataloader()))[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from chop.passes.graph.utils import get_parent_name\n",
    "\n",
    "# 1. Can you edit your code, so that we can modify the above network to have layers expanded to double their sizes? Note: you will have to change the ReLU also.\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),  # unchanged      \n",
    "            nn.ReLU(16),     \n",
    "            nn.Linear(16, 16), # output      \n",
    "            nn.ReLU(16),     \n",
    "            nn.Linear(16, 16), # input & output     \n",
    "            nn.ReLU(16),        \n",
    "            nn.Linear(16, 5),  # input    \n",
    "            nn.ReLU(5),       \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for the network transform (i.e, channel multiplier)\n",
    "\n",
    "import copy\n",
    "\n",
    "def instantiate_linear(in_features, out_features, bias):\n",
    "    if bias is not None:\n",
    "        bias = True\n",
    "    return nn.Linear(\n",
    "        in_features=in_features, \n",
    "        out_features=out_features, \n",
    "        bias=bias)\n",
    "\n",
    "def redefine_linear_transform_pass(graph, pass_args=None):\n",
    "    pass_args_copy = copy.deepcopy(pass_args)\n",
    "    main_config = pass_args_copy.pop('config')\n",
    "    print(\"main_config\",main_config)\n",
    "    \n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "        raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "        if name is not None:\n",
    "            ori_module = graph.modules[node.target]\n",
    "            in_features = ori_module.in_features\n",
    "            out_features = ori_module.out_features\n",
    "            bias = ori_module.bias\n",
    "            if name == \"output_only\":\n",
    "                out_features = out_features * config[\"channel_multiplier\"] \n",
    "            elif name == \"both\":\n",
    "                in_features = in_features * config[\"channel_multiplier\"] \n",
    "                out_features = out_features * config[\"channel_multiplier\"] \n",
    "            elif name == \"input_only\":\n",
    "                in_features = in_features * config[\"channel_multiplier\"] \n",
    "                # bias unchanged\n",
    "            new_module = instantiate_linear(in_features, out_features, bias)\n",
    "            parent_name, name = get_parent_name(node.target)\n",
    "            setattr(graph.modules[parent_name], name, new_module)\n",
    "    return graph, {}\n",
    "\n",
    "\n",
    "def instantiate_relu(boolean):\n",
    "    return nn.ReLU(inplace=boolean)\n",
    "\n",
    "def redefine_relu_pass(graph, pass_args=None):\n",
    "    pass_args_copy = copy.deepcopy(pass_args)\n",
    "    main_config = pass_args_copy.pop('config')\n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "        raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "        if name is not None:\n",
    "            new_module = instantiate_relu(True)\n",
    "            parent_name, name = get_parent_name(node.target)\n",
    "            setattr(graph.modules[parent_name], name, new_module)\n",
    "    return graph, {}\n",
    "    \n",
    "\n",
    "pass_config_linear = {\n",
    "\"by\": \"name\",\n",
    "\"default\": {\"config\": {\"name\": None}},\n",
    "\"seq_blocks_2\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"output_only\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\"seq_blocks_4\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"both\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\"seq_blocks_6\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"input_only\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pass_config_relu = {\n",
    "\"by\": \"name\",\n",
    "\"default\": {\"config\": {\"name\": None}},\n",
    "\"seq_blocks_3\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"relu\",\n",
    "        }\n",
    "    },\n",
    "\"seq_blocks_5\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"relu\",\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 2}}}\n"
     ]
    }
   ],
   "source": [
    "model = JSC_Three_Linear_Layers()\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "\n",
    "mg, _ = redefine_linear_transform_pass(graph=mg, pass_args={\"config\": pass_config_linear})\n",
    "mg, _ = redefine_relu_pass(graph=mg, pass_args={\"config\": pass_config_relu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (seq_blocks): Module(\n",
       "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom torchmetrics.classification import MulticlassAccuracy\\nfrom torch import optim\\n\\ndef init_mg():\\n    model = JSC_Three_Linear_Layers()\\n    mg = MaseGraph(model=model)\\n    mg, _ = init_metadata_analysis_pass(mg, None)\\n    return mg\\n\\nchannel_multiplier = [1,2,3,4,5]\\nmax_epoch=10\\n\\nbatch_size = 512\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\nfor multiplier in channel_multiplier:\\n    mg = init_mg()\\n    pass_config_linear[\"seq_blocks_2\"][\"config\"][\"channel_multiplier\"] = multiplier\\n    pass_config_linear[\"seq_blocks_4\"][\"config\"][\"channel_multiplier\"] = multiplier\\n    pass_config_linear[\"seq_blocks_6\"][\"config\"][\"channel_multiplier\"] = multiplier\\n\\n    mg, _ = redefine_linear_transform_pass(graph=mg, pass_args={\"config\": pass_config_linear})\\n    mg, _ = redefine_relu_pass(graph=mg, pass_args={\"config\": pass_config_relu})\\n\\n    for epoch in range(max_epoch):\\n        for inputs in data_module.train_dataloader():\\n            xs, ys = inputs\\n            optimizer.zero_grad()\\n            preds = mg.model(xs)\\n            loss = torch.nn.functional.cross_entropy(preds, ys)  \\n            loss.backward()  \\n            optimizer.step() \\n\\n    torch.save({\\n        \\'state_dict\\': mg.model.state_dict(),\\n        \\'config\\': multiplier,\\n    }, f\\'model_with_multiplier_{str(multiplier)}.pth\\')\\n\\n    mg = init_mg(model)\\n    optimizer = optim.Adam(mg.model.parameters(), lr=0.001)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Grid Search for the best channel multiplier value\n",
    "\n",
    "# train\n",
    "\n",
    "'''\n",
    "import torch\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torch import optim\n",
    "\n",
    "def init_mg():\n",
    "    model = JSC_Three_Linear_Layers()\n",
    "    mg = MaseGraph(model=model)\n",
    "    mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "    return mg\n",
    "\n",
    "channel_multiplier = [1,2,3,4,5]\n",
    "max_epoch=10\n",
    "\n",
    "batch_size = 512\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for multiplier in channel_multiplier:\n",
    "    mg = init_mg()\n",
    "    pass_config_linear[\"seq_blocks_2\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "    pass_config_linear[\"seq_blocks_4\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "    pass_config_linear[\"seq_blocks_6\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "\n",
    "    mg, _ = redefine_linear_transform_pass(graph=mg, pass_args={\"config\": pass_config_linear})\n",
    "    mg, _ = redefine_relu_pass(graph=mg, pass_args={\"config\": pass_config_relu})\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        for inputs in data_module.train_dataloader():\n",
    "            xs, ys = inputs\n",
    "            optimizer.zero_grad()\n",
    "            preds = mg.model(xs)\n",
    "            loss = torch.nn.functional.cross_entropy(preds, ys)  \n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "\n",
    "    torch.save({\n",
    "        'state_dict': mg.model.state_dict(),\n",
    "        'config': multiplier,\n",
    "    }, f'model_with_multiplier_{str(multiplier)}.pth')\n",
    "\n",
    "    mg = init_mg(model)\n",
    "    optimizer = optim.Adam(mg.model.parameters(), lr=0.001)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_2/model_with_multiplier_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_2/model_with_multiplier_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_2/model_with_multiplier_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_2/model_with_multiplier_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_2/model_with_multiplier_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier': 5}}}\n"
     ]
    }
   ],
   "source": [
    "# 2. Grid Search for the best channel multiplier value\n",
    "\n",
    "# search\n",
    "\n",
    "import torch\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import gc\n",
    "\n",
    "def init_mg():\n",
    "    model = JSC_Three_Linear_Layers()\n",
    "    mg = MaseGraph(model=model)\n",
    "    mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "    return mg\n",
    "\n",
    "channel_multiplier = [1,2,3,4,5]\n",
    "recorded_accs = []\n",
    "metric = MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "for multiplier in channel_multiplier:\n",
    "    mg = init_mg()\n",
    "    pass_config_linear[\"seq_blocks_2\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "    pass_config_linear[\"seq_blocks_4\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "    pass_config_linear[\"seq_blocks_6\"][\"config\"][\"channel_multiplier\"] = multiplier\n",
    "\n",
    "    mg, _ = redefine_linear_transform_pass(graph=mg, pass_args={\"config\": pass_config_linear})\n",
    "    mg, _ = redefine_relu_pass(graph=mg, pass_args={\"config\": pass_config_relu})\n",
    "\n",
    "    mymodel = load_model(f\"mase_output/4_2/model_with_multiplier_{multiplier}.ckpt\", \"pl\", mg.model)\n",
    "\n",
    "    acc_avg, loss_avg = 0, 0 \n",
    "    accs, losses = [], []\n",
    "    for inputs in data_module.train_dataloader():\n",
    "        xs, ys = inputs\n",
    "        preds = mymodel(xs)\n",
    "        acc = metric(preds, ys)\n",
    "        accs.append(acc)\n",
    "        loss = torch.nn.functional.cross_entropy(preds, ys)\n",
    "        losses.append(loss)\n",
    "\n",
    "    acc_avg = sum(accs) / len(accs)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    recorded_accs.append({\"multiplier\":multiplier, \"acc\":acc_avg.item(),\"loss\":loss_avg.item()})\n",
    "\n",
    "    del mymodel, mg, accs, losses, acc_avg, loss_avg\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multiplier</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.185062</td>\n",
       "      <td>1.615053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.205691</td>\n",
       "      <td>1.611572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.118622</td>\n",
       "      <td>1.619060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.123247</td>\n",
       "      <td>1.607118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.234985</td>\n",
       "      <td>1.604752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   multiplier       acc      loss\n",
       "0           1  0.185062  1.615053\n",
       "1           2  0.205691  1.611572\n",
       "2           3  0.118622  1.619060\n",
       "3           4  0.123247  1.607118\n",
       "4           5  0.234985  1.604752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(recorded_accs)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultipliers = [1, 2, 3, 4, 5]\\n\\nmax_epoch=10\\nbatch_size = 512\\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\\n\\nfor a in multipliers:\\n    for c in multipliers:\\n        b = a\\n        d = c\\n        pass_config_linear = design_pass_config_linear(a, b, c, d)\\n\\n        mg = init_mg()\\n        mg, _ = redefine_linear_transform_pass(mg, pass_args={\"config\": pass_config_linear})\\n        mg, _ = redefine_relu_pass(mg, pass_args={\"config\": pass_config_relu})\\n\\n        for epoch in range(max_epoch):\\n            for inputs in data_module.train_dataloader():\\n                xs, ys = inputs\\n                optimizer.zero_grad()\\n                preds = mg.model(xs)\\n                loss = torch.nn.functional.cross_entropy(preds, ys)  \\n                loss.backward()  \\n                optimizer.step() \\n\\n        torch.save({\\n            \\'state_dict\\': mg.model.state_dict(),\\n            \\'config\\': [a,b,c,d],\\n        }, f\\'model_with_multiplier_{a}_{b}_{c}_{d}.ckpt\\')\\n\\n        mg = init_mg(model)\\n        optimizer = optim.Adam(mg.model.parameters(), lr=1e-5)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. More advanced search\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# rewrite linear_transform\n",
    "def redefine_linear_transform_pass(graph, pass_args=None):\n",
    "    pass_args_copy = copy.deepcopy(pass_args)\n",
    "    main_config = pass_args_copy.pop('config')\n",
    "    print(\"main_config\",main_config)\n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "        raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "        if name is not None:\n",
    "            ori_module = graph.modules[node.target]\n",
    "            in_features = ori_module.in_features   \n",
    "            out_features = ori_module.out_features\n",
    "            bias = ori_module.bias\n",
    "            if name == \"output_only\":\n",
    "                out_features = out_features * config[\"channel_multiplier_output\"] \n",
    "            elif name == \"both\":\n",
    "                in_features = in_features * config[\"channel_multiplier_input\"] \n",
    "                out_features = out_features * config[\"channel_multiplier_output\"] \n",
    "            elif name == \"input_only\":\n",
    "                in_features = in_features * config[\"channel_multiplier_input\"] \n",
    "                # bias unchanged\n",
    "            new_module = instantiate_linear(in_features, out_features, bias)\n",
    "            parent_name, name = get_parent_name(node.target)\n",
    "            setattr(graph.modules[parent_name], name, new_module)\n",
    "    return graph, {}\n",
    "\n",
    "def init_mg():\n",
    "    model = JSC_Three_Linear_Layers()\n",
    "    mg = MaseGraph(model=model)\n",
    "    mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "    return mg\n",
    "\n",
    "def design_pass_config_linear(a,b,c,d):\n",
    "    config = {\n",
    "        \"by\": \"name\",\n",
    "        \"default\": {\"config\": {\"name\": None}},\n",
    "        \"seq_blocks_2\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"output_only\",\n",
    "                \"channel_multiplier_output\": a,\n",
    "            }\n",
    "        },\n",
    "        \"seq_blocks_4\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"both\",\n",
    "                \"channel_multiplier_input\": b,\n",
    "                \"channel_multiplier_output\": c,\n",
    "            }\n",
    "        },\n",
    "        \"seq_blocks_6\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"input_only\",\n",
    "                \"channel_multiplier_input\": d,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "'''\n",
    "multipliers = [1, 2, 3, 4, 5]\n",
    "\n",
    "max_epoch=10\n",
    "batch_size = 512\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for a in multipliers:\n",
    "    for c in multipliers:\n",
    "        b = a\n",
    "        d = c\n",
    "        pass_config_linear = design_pass_config_linear(a, b, c, d)\n",
    "\n",
    "        mg = init_mg()\n",
    "        mg, _ = redefine_linear_transform_pass(mg, pass_args={\"config\": pass_config_linear})\n",
    "        mg, _ = redefine_relu_pass(mg, pass_args={\"config\": pass_config_relu})\n",
    "\n",
    "        for epoch in range(max_epoch):\n",
    "            for inputs in data_module.train_dataloader():\n",
    "                xs, ys = inputs\n",
    "                optimizer.zero_grad()\n",
    "                preds = mg.model(xs)\n",
    "                loss = torch.nn.functional.cross_entropy(preds, ys)  \n",
    "                loss.backward()  \n",
    "                optimizer.step() \n",
    "\n",
    "        torch.save({\n",
    "            'state_dict': mg.model.state_dict(),\n",
    "            'config': [a,b,c,d],\n",
    "        }, f'model_with_multiplier_{a}_{b}_{c}_{d}.ckpt')\n",
    "\n",
    "        mg = init_mg(model)\n",
    "        optimizer = optim.Adam(mg.model.parameters(), lr=1e-5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n",
      "acc_1_1_1_1 tensor(0.2019)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n",
      "acc_1_1_2_2 tensor(0.1955)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n",
      "acc_1_1_3_3 tensor(0.2063)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n",
      "acc_1_1_4_4 tensor(0.1868)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n",
      "acc_1_1_5_5 tensor(0.1376)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n",
      "acc_2_2_1_1 tensor(0.1947)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n",
      "acc_2_2_2_2 tensor(0.1251)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n",
      "acc_2_2_3_3 tensor(0.2000)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n",
      "acc_2_2_4_4 tensor(0.2575)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n",
      "acc_2_2_5_5 tensor(0.1999)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n",
      "acc_3_3_1_1 tensor(0.1454)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n",
      "acc_3_3_2_2 tensor(0.2772)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n",
      "acc_3_3_3_3 tensor(0.1628)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n",
      "acc_3_3_4_4 tensor(0.2000)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n",
      "acc_3_3_5_5 tensor(0.2053)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n",
      "acc_4_4_1_1 tensor(0.2225)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n",
      "acc_4_4_2_2 tensor(0.1461)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n",
      "acc_4_4_3_3 tensor(0.2000)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n",
      "acc_4_4_4_4 tensor(0.1356)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n",
      "acc_4_4_5_5 tensor(0.2132)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n",
      "acc_5_5_1_1 tensor(0.2020)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n",
      "acc_5_5_2_2 tensor(0.1779)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n",
      "acc_5_5_3_3 tensor(0.2296)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n",
      "acc_5_5_4_4 tensor(0.2000)\n",
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n",
      "acc_5_5_5_5 tensor(0.1906)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "multipliers = [1, 2, 3, 4, 5]\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "metric = MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "max_epoch=10\n",
    "batch_size = 512\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for a in multipliers:\n",
    "    for c in multipliers:\n",
    "        b = a\n",
    "        d = c\n",
    "        pass_config_linear = design_pass_config_linear(a, b, c, d)\n",
    "\n",
    "        mg = init_mg()\n",
    "        mg, _ = redefine_linear_transform_pass(mg, pass_args={\"config\": pass_config_linear})\n",
    "        mg, _ = redefine_relu_pass(mg, pass_args={\"config\": pass_config_relu})\n",
    "\n",
    "        for epoch in range(max_epoch):\n",
    "            data_iterator = iter(data_module.train_dataloader())\n",
    "            inputs = next(data_iterator)\n",
    "            xs, ys = inputs\n",
    "            optimizer.zero_grad()\n",
    "            preds = mg.model(xs)\n",
    "            loss = torch.nn.functional.cross_entropy(preds, ys)  \n",
    "            loss.backward()  \n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs in data_module.train_dataloader():\n",
    "                xs, ys = inputs\n",
    "                preds = mg.model(xs)\n",
    "                acc = metric(preds, ys)\n",
    "                loss = torch.nn.functional.cross_entropy(preds, ys)\n",
    "\n",
    "        print(f\"acc_{a}_{b}_{c}_{d}\",acc)\n",
    "\n",
    "        mg = init_mg()\n",
    "        optimizer = optim.Adam(mg.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_1_1_1_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_1_1_2_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_1_1_3_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_1_1_5_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 1}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 1, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_2_2_1_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_2_2_2_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_2_2_3_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_2_2_4_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_2_2_5_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 2}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 2, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_3_3_1_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_3_3_2_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_3_3_3_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_3_3_4_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_3_3_5_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 3}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 3, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_4_4_1_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_4_4_2_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_4_4_3_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_4_4_4_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_4_4_5_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 4}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 4, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_5_5_1_1.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 1}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 1}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_5_5_2_2.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 2}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 2}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_5_5_3_3.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 3}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 3}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_5_5_4_4.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 4}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 4}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from mase_output/4_3/model_with_multiplier_5_5_5_5.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_config {'by': 'name', 'default': {'config': {'name': None}}, 'seq_blocks_2': {'config': {'name': 'output_only', 'channel_multiplier_output': 5}}, 'seq_blocks_4': {'config': {'name': 'both', 'channel_multiplier_input': 5, 'channel_multiplier_output': 5}}, 'seq_blocks_6': {'config': {'name': 'input_only', 'channel_multiplier_input': 5}}}\n"
     ]
    }
   ],
   "source": [
    "# 3. More advanced search\n",
    "\n",
    "# search\n",
    "\n",
    "import torch\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import gc\n",
    "\n",
    "multipliers = [1, 2, 3, 4, 5]\n",
    "\n",
    "recorded_metrics = []\n",
    "metric = MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "for a in multipliers:\n",
    "    for c in multipliers:\n",
    "        b = a\n",
    "        d = c\n",
    "        pass_config_linear = design_pass_config_linear(a, b, c, d)\n",
    "\n",
    "        mg = init_mg()\n",
    "        mg, _ = redefine_linear_transform_pass(mg, pass_args={\"config\": pass_config_linear})\n",
    "        mg, _ = redefine_relu_pass(mg, pass_args={\"config\": pass_config_relu})\n",
    "\n",
    "        mymodel = load_model(f\"mase_output/4_3/model_with_multiplier_{a}_{b}_{c}_{d}.ckpt\", \"pl\", mg.model)\n",
    "\n",
    "        acc_avg, loss_avg = 0, 0\n",
    "        accs, losses = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs in data_module.train_dataloader():\n",
    "                xs, ys = inputs\n",
    "                preds = mg.model(xs)\n",
    "                acc = metric(preds, ys)\n",
    "                accs.append(acc.item())\n",
    "                loss = torch.nn.functional.cross_entropy(preds, ys)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        acc_avg = sum(accs) / len(accs)\n",
    "        loss_avg = sum(losses) / len(losses)\n",
    "        recorded_metrics.append({\"block2_output\":a, \"block4_input\":b, \"block4_output\":c, \"block6_input\":d, \"acc(%)\":acc_avg*100, \"loss\":loss_avg})\n",
    "\n",
    "        del mymodel, mg, accs, losses, acc_avg, loss_avg\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block2_output</th>\n",
       "      <th>block4_input</th>\n",
       "      <th>block4_output</th>\n",
       "      <th>block6_input</th>\n",
       "      <th>acc(%)</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.629313</td>\n",
       "      <td>1.607766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22.239536</td>\n",
       "      <td>1.603152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20.041148</td>\n",
       "      <td>1.603113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>32.193983</td>\n",
       "      <td>1.597404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>16.924062</td>\n",
       "      <td>1.612282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.999228</td>\n",
       "      <td>1.617489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.454291</td>\n",
       "      <td>1.614191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>21.310789</td>\n",
       "      <td>1.614070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19.583254</td>\n",
       "      <td>1.604264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>22.100852</td>\n",
       "      <td>1.601225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.622889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.103185</td>\n",
       "      <td>1.605774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>18.963363</td>\n",
       "      <td>1.612705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20.045092</td>\n",
       "      <td>1.611409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12.229278</td>\n",
       "      <td>1.616797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.900911</td>\n",
       "      <td>1.609808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.799091</td>\n",
       "      <td>1.613384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17.733875</td>\n",
       "      <td>1.612702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>25.527337</td>\n",
       "      <td>1.614455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13.759217</td>\n",
       "      <td>1.621978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.743408</td>\n",
       "      <td>1.611521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22.996250</td>\n",
       "      <td>1.615066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20.006415</td>\n",
       "      <td>1.602762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20.297489</td>\n",
       "      <td>1.614881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23.279639</td>\n",
       "      <td>1.611035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    block2_output  block4_input  block4_output  block6_input     acc(%)  \\\n",
       "0               1             1              1             1  20.629313   \n",
       "1               1             1              2             2  22.239536   \n",
       "2               1             1              3             3  20.041148   \n",
       "3               1             1              4             4  32.193983   \n",
       "4               1             1              5             5  16.924062   \n",
       "5               2             2              1             1  19.999228   \n",
       "6               2             2              2             2  20.454291   \n",
       "7               2             2              3             3  21.310789   \n",
       "8               2             2              4             4  19.583254   \n",
       "9               2             2              5             5  22.100852   \n",
       "10              3             3              1             1  20.000000   \n",
       "11              3             3              2             2  18.103185   \n",
       "12              3             3              3             3  18.963363   \n",
       "13              3             3              4             4  20.045092   \n",
       "14              3             3              5             5  12.229278   \n",
       "15              4             4              1             1  20.900911   \n",
       "16              4             4              2             2  13.799091   \n",
       "17              4             4              3             3  17.733875   \n",
       "18              4             4              4             4  25.527337   \n",
       "19              4             4              5             5  13.759217   \n",
       "20              5             5              1             1  20.743408   \n",
       "21              5             5              2             2  22.996250   \n",
       "22              5             5              3             3  20.006415   \n",
       "23              5             5              4             4  20.297489   \n",
       "24              5             5              5             5  23.279639   \n",
       "\n",
       "        loss  \n",
       "0   1.607766  \n",
       "1   1.603152  \n",
       "2   1.603113  \n",
       "3   1.597404  \n",
       "4   1.612282  \n",
       "5   1.617489  \n",
       "6   1.614191  \n",
       "7   1.614070  \n",
       "8   1.604264  \n",
       "9   1.601225  \n",
       "10  1.622889  \n",
       "11  1.605774  \n",
       "12  1.612705  \n",
       "13  1.611409  \n",
       "14  1.616797  \n",
       "15  1.609808  \n",
       "16  1.613384  \n",
       "17  1.612702  \n",
       "18  1.614455  \n",
       "19  1.621978  \n",
       "20  1.611521  \n",
       "21  1.615066  \n",
       "22  1.602762  \n",
       "23  1.614881  \n",
       "24  1.611035  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recorded_metrics\n",
    "\n",
    "metrics = pd.DataFrame(recorded_metrics)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/imperial/second_term/adls/rs1923/mase_real/machop\n"
     ]
    }
   ],
   "source": [
    "%cd machop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of JSC_1923 parameters: 3285\n",
      "Total number of JSC_Tiny parameters: 117\n",
      "Seed set to 0\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          |       Config. File       |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |           cls            |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           | \u001b[38;5;8m/mnt/d/imperial/second_t\u001b[0m | /mnt/d/imperial/second_t | /mnt/d/imperial/second_t |\n",
      "|                         |                          | \u001b[38;5;8merm/adls/rs1923/mase_rea\u001b[0m | erm/adls/rs1923/mase_rea | erm/adls/rs1923/mase_rea |\n",
      "|                         |                          | \u001b[38;5;8ml/mase_output/jsc-three-\u001b[0m | l/mase_output/jsc-three- | l/mase_output/jsc-three- |\n",
      "|                         |                          | \u001b[38;5;8mlinear-layers_classifica\u001b[0m | linear-layers_classifica | linear-layers_classifica |\n",
      "|                         |                          | \u001b[38;5;8mtion_jsc_2024-02-10/soft\u001b[0m | tion_jsc_2024-02-10/soft | tion_jsc_2024-02-10/soft |\n",
      "|                         |                          | \u001b[38;5;8mware/training_ckpts/best\u001b[0m | ware/training_ckpts/best | ware/training_ckpts/best |\n",
      "|                         |                          |          \u001b[38;5;8m.ckpt\u001b[0m           |          .ckpt           |          .ckpt           |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |            pl            |                          |            pl            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |           512            |                          |           512            |\n",
      "| to_debug                |          False           |                          |                          |          False           |\n",
      "| log_level               |           info           |                          |                          |           info           |\n",
      "| report_to               |       tensorboard        |                          |                          |       tensorboard        |\n",
      "| seed                    |            \u001b[38;5;8m0\u001b[0m             |            42            |                          |            42            |\n",
      "| quant_config            |           None           |                          |                          |           None           |\n",
      "| training_optimizer      |           adam           |                          |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |                          |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |          1e-05           |                          |          1e-05           |\n",
      "| weight_decay            |            0             |                          |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |            5             |                          |            5             |\n",
      "| max_steps               |            -1            |                          |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |                          |                          |            1             |\n",
      "| log_every_n_steps       |            \u001b[38;5;8m50\u001b[0m            |            5             |                          |            5             |\n",
      "| num_workers             |            20            |                          |                          |            20            |\n",
      "| num_devices             |            1             |                          |                          |            1             |\n",
      "| num_nodes               |            1             |                          |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |           cpu            |                          |           cpu            |\n",
      "| strategy                |           auto           |                          |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |                          |                          |          False           |\n",
      "| github_ci               |          False           |                          |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |                          |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |                          |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |                          |                          |           100            |\n",
      "| is_pretrained           |          False           |                          |                          |          False           |\n",
      "| max_token_len           |           512            |                          |                          |           512            |\n",
      "| project_dir             | /mnt/d/imperial/second_t |                          |                          | /mnt/d/imperial/second_t |\n",
      "|                         | erm/adls/rs1923/mase_rea |                          |                          | erm/adls/rs1923/mase_rea |\n",
      "|                         |      l/mase_output       |                          |                          |      l/mase_output       |\n",
      "| project                 |           \u001b[38;5;8mNone\u001b[0m           |      network_search      |                          |      network_search      |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           | jsc-three-linear-layers  |                          | jsc-three-linear-layers  |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |           jsc            |                          |           jsc            |\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'jsc-three-linear-layers'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'jsc'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/network_search\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/jsc-three-linear-layers_classification_jsc_2024-02-10/software/training_ckpts/best.ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded model from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/jsc-three-linear-layers_classification_jsc_2024-02-10/software/training_ckpts/best.ckpt.\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBuilding search space...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSearch started...\u001b[0m\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  3%|▊                       | 1/30 [00:00<00:13,  2.11it/s, 0.47/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_4_4_5_5.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=80, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  7%|█▌                      | 2/30 [00:00<00:12,  2.31it/s, 0.88/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_5_5_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=80, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=80, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 10%|██▍                     | 3/30 [00:01<00:11,  2.33it/s, 1.30/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_5_5_5_5.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=80, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=80, out_features=80, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 13%|███▏                    | 4/30 [00:01<00:10,  2.39it/s, 1.71/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_2_2.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 17%|████                    | 5/30 [00:02<00:10,  2.38it/s, 2.13/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_4_4_2_2.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 20%|████▊                   | 6/30 [00:02<00:10,  2.39it/s, 2.54/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_4_4_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 23%|█████▌                  | 7/30 [00:02<00:09,  2.40it/s, 2.96/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_4_4_5_5.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=80, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 27%|██████▍                 | 8/30 [00:03<00:09,  2.41it/s, 3.37/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_5_5_3_3.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=80, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=80, out_features=48, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=48, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 30%|███████▏                | 9/30 [00:03<00:08,  2.44it/s, 3.77/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 33%|███████▋               | 10/30 [00:04<00:08,  2.41it/s, 4.19/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_3_3.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=48, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 37%|████████▍              | 11/30 [00:04<00:07,  2.39it/s, 4.62/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_2_2.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 40%|█████████▏             | 12/30 [00:05<00:07,  2.43it/s, 5.01/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_2_2.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 43%|█████████▉             | 13/30 [00:05<00:07,  2.36it/s, 5.47/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 47%|██████████▋            | 14/30 [00:05<00:06,  2.36it/s, 5.89/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 50%|███████████▌           | 15/30 [00:06<00:06,  2.30it/s, 6.35/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_2_2.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=32, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 53%|████████████▎          | 16/30 [00:06<00:06,  2.32it/s, 6.77/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_1_1.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 57%|█████████████          | 17/30 [00:07<00:05,  2.28it/s, 7.23/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 60%|█████████████▊         | 18/30 [00:07<00:05,  2.24it/s, 7.69/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 63%|██████████████▌        | 19/30 [00:08<00:04,  2.24it/s, 8.14/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 67%|███████████████▎       | 20/30 [00:08<00:04,  2.30it/s, 8.55/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 70%|████████████████       | 21/30 [00:08<00:03,  2.32it/s, 8.97/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 73%|████████████████▊      | 22/30 [00:09<00:03,  2.33it/s, 9.40/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 77%|█████████████████▋     | 23/30 [00:09<00:03,  2.33it/s, 9.82/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 80%|█████████████████▌    | 24/30 [00:10<00:02,  2.39it/s, 10.22/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_3_3.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=48, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=48, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 83%|██████████████████▎   | 25/30 [00:10<00:02,  2.36it/s, 10.65/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_5_5.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=80, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 87%|███████████████████   | 26/30 [00:11<00:01,  2.31it/s, 11.11/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_2_2_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 90%|███████████████████▊  | 27/30 [00:11<00:01,  2.31it/s, 11.54/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_3_3_3_3.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=48, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=48, out_features=48, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=48, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 93%|████████████████████▌ | 28/30 [00:11<00:00,  2.31it/s, 11.97/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_4_4_4_4.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 97%|█████████████████████▎| 29/30 [00:12<00:00,  2.26it/s, 12.43/20000 seconds]\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/4_3/model_with_multiplier_1_1_5_5.ckpt\u001b[0m\n",
      "GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=16, out_features=80, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=80, out_features=5, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    return seq_blocks_7\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "100%|██████████████████████| 30/30 [00:12<00:00,  2.33it/s, 12.90/20000 seconds]\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBest trial(s):\n",
      "Best trial(s):\n",
      "|    |   number | software_metrics                  | hardware_metrics                                | scaled_metrics                               |\n",
      "|----+----------+-----------------------------------+-------------------------------------------------+----------------------------------------------|\n",
      "|  0 |       21 | {'loss': 1.61, 'accuracy': 0.221} | {'average_bitwidth': 32, 'memory_density': 1.0} | {'accuracy': 0.221, 'average_bitwidth': 6.4} |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSearching is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 4. Integrate the search to the chop flow, so we can run it from the command line.\n",
    "\n",
    "!./ch search --config configs/examples/jsc_network_search.toml --load /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/jsc-three-linear-layers_classification_jsc_2024-02-10/software/training_ckpts/best.ckpt\n",
    "# comparison on the performance: using xxx & not using xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/imperial/second_term/adls/rs1923/mase_real/machop\n"
     ]
    }
   ],
   "source": [
    "%cd machop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of JSC_1923 parameters: 3285\n",
      "Total number of JSC_Tiny parameters: 117\n",
      "Seed set to 0\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "| Name                    |         Default          |       Config. File       |     Manual Override      |        Effective         |\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "| task                    |      \u001b[38;5;8mclassification\u001b[0m      |           cls            |                          |           cls            |\n",
      "| load_name               |           \u001b[38;5;8mNone\u001b[0m           | \u001b[38;5;8m../mase_output/vgg7_clas\u001b[0m | /mnt/d/imperial/second_t | /mnt/d/imperial/second_t |\n",
      "|                         |                          | \u001b[38;5;8msification_cifar10_2024-\u001b[0m | erm/adls/rs1923/mase_rea | erm/adls/rs1923/mase_rea |\n",
      "|                         |                          | \u001b[38;5;8m02-01/software/training_\u001b[0m | l/mase_output/vgg7_class | l/mase_output/vgg7_class |\n",
      "|                         |                          |     \u001b[38;5;8mckpts/best.ckpt\u001b[0m      | ification_cifar10_2024-0 | ification_cifar10_2024-0 |\n",
      "|                         |                          |                          | 2-01/software/training_c | 2-01/software/training_c |\n",
      "|                         |                          |                          |      kpts/best.ckpt      |      kpts/best.ckpt      |\n",
      "| load_type               |            \u001b[38;5;8mmz\u001b[0m            |            pt            |                          |            pt            |\n",
      "| batch_size              |           \u001b[38;5;8m128\u001b[0m            |           512            |                          |           512            |\n",
      "| to_debug                |          False           |                          |                          |          False           |\n",
      "| log_level               |           info           |                          |                          |           info           |\n",
      "| report_to               |       tensorboard        |                          |                          |       tensorboard        |\n",
      "| seed                    |            \u001b[38;5;8m0\u001b[0m             |            42            |                          |            42            |\n",
      "| quant_config            |           None           |                          |                          |           None           |\n",
      "| training_optimizer      |           adam           |                          |                          |           adam           |\n",
      "| trainer_precision       |         16-mixed         |                          |                          |         16-mixed         |\n",
      "| learning_rate           |          \u001b[38;5;8m1e-05\u001b[0m           |          1e-05           |                          |          1e-05           |\n",
      "| weight_decay            |            0             |                          |                          |            0             |\n",
      "| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |            5             |                          |            5             |\n",
      "| max_steps               |            -1            |                          |                          |            -1            |\n",
      "| accumulate_grad_batches |            1             |                          |                          |            1             |\n",
      "| log_every_n_steps       |            \u001b[38;5;8m50\u001b[0m            |            5             |                          |            5             |\n",
      "| num_workers             |            20            |                          |                          |            20            |\n",
      "| num_devices             |            1             |                          |                          |            1             |\n",
      "| num_nodes               |            1             |                          |                          |            1             |\n",
      "| accelerator             |           \u001b[38;5;8mauto\u001b[0m           |           cpu            |                          |           cpu            |\n",
      "| strategy                |           auto           |                          |                          |           auto           |\n",
      "| is_to_auto_requeue      |          False           |                          |                          |          False           |\n",
      "| github_ci               |          False           |                          |                          |          False           |\n",
      "| disable_dataset_cache   |          False           |                          |                          |          False           |\n",
      "| target                  |   xcu250-figd2104-2L-e   |                          |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |           100            |                          |                          |           100            |\n",
      "| is_pretrained           |          False           |                          |                          |          False           |\n",
      "| max_token_len           |           512            |                          |                          |           512            |\n",
      "| project_dir             | /mnt/d/imperial/second_t |                          |                          | /mnt/d/imperial/second_t |\n",
      "|                         | erm/adls/rs1923/mase_rea |                          |                          | erm/adls/rs1923/mase_rea |\n",
      "|                         |      l/mase_output       |                          |                          |      l/mase_output       |\n",
      "| project                 |           \u001b[38;5;8mNone\u001b[0m           |       cifar10_vgg        |                          |       cifar10_vgg        |\n",
      "| model                   |           \u001b[38;5;8mNone\u001b[0m           |           vgg7           |                          |           vgg7           |\n",
      "| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |         cifar10          |                          |         cifar10          |\n",
      "+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/cifar10_vgg\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch checkpoint from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/vgg7_classification_cifar10_2024-02-01/software/training_ckpts/best.ckpt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded model from /mnt/d/imperial/second_term/adls/rs1923/mase_real/mase_output/vgg7_classification_cifar10_2024-02-01/software/training_ckpts/best.ckpt.\u001b[0m\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBuilding search space...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSearch started...\u001b[0m\n",
      "  0%|                                                    | 0/90 [00:00<?, ?it/s]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  1%|▎                       | 1/90 [00:06<10:20,  6.97s/it, 6.97/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  2%|▌                      | 2/90 [00:11<08:14,  5.62s/it, 11.65/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  3%|▊                      | 3/90 [00:22<11:45,  8.10s/it, 22.71/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  4%|█                      | 4/90 [00:38<16:00, 11.16s/it, 38.56/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  6%|█▎                     | 5/90 [00:42<12:08,  8.57s/it, 42.53/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  7%|█▌                     | 6/90 [00:49<11:14,  8.03s/it, 49.51/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  8%|█▊                     | 7/90 [00:56<10:50,  7.83s/it, 56.94/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "  9%|██                     | 8/90 [01:01<09:15,  6.78s/it, 61.46/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 10%|██▎                    | 9/90 [01:07<08:50,  6.55s/it, 67.51/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 11%|██▍                   | 10/90 [01:15<09:13,  6.91s/it, 75.23/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 12%|██▋                   | 11/90 [01:23<09:44,  7.40s/it, 83.72/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 13%|██▊                  | 12/90 [01:46<15:48, 12.16s/it, 106.78/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 14%|███                  | 13/90 [01:54<13:53, 10.83s/it, 114.55/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 16%|███▎                 | 14/90 [02:00<11:55,  9.41s/it, 120.68/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 17%|███▌                 | 15/90 [02:27<18:23, 14.71s/it, 147.66/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 18%|███▋                 | 16/90 [02:34<15:02, 12.20s/it, 154.05/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 19%|███▉                 | 17/90 [02:41<13:15, 10.89s/it, 161.89/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 20%|████▏                | 18/90 [02:57<14:46, 12.31s/it, 177.51/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 21%|████▍                | 19/90 [03:08<14:10, 11.97s/it, 188.70/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 22%|████▋                | 20/90 [03:38<20:13, 17.34s/it, 218.54/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 23%|████▉                | 21/90 [03:43<15:37, 13.59s/it, 223.37/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 24%|█████▏               | 22/90 [03:47<12:16, 10.82s/it, 227.76/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 26%|█████▎               | 23/90 [03:54<10:42,  9.59s/it, 234.47/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 27%|█████▌               | 24/90 [04:03<10:31,  9.56s/it, 243.97/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 28%|█████▊               | 25/90 [04:12<09:52,  9.11s/it, 252.03/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 29%|██████               | 26/90 [04:42<16:39, 15.61s/it, 282.81/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 30%|██████▎              | 27/90 [04:49<13:40, 13.03s/it, 289.81/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 31%|██████▌              | 28/90 [05:11<16:18, 15.77s/it, 311.99/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 32%|██████▊              | 29/90 [05:19<13:35, 13.37s/it, 319.75/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 33%|███████              | 30/90 [05:35<14:07, 14.12s/it, 335.63/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 34%|███████▏             | 31/90 [06:06<18:52, 19.20s/it, 366.67/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 36%|███████▍             | 32/90 [06:12<14:44, 15.26s/it, 372.73/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 37%|███████▋             | 33/90 [06:23<13:11, 13.89s/it, 383.44/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 38%|███████▉             | 34/90 [06:28<10:29, 11.24s/it, 388.48/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 39%|████████▏            | 35/90 [06:37<09:33, 10.43s/it, 397.03/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 40%|████████▍            | 36/90 [06:41<07:45,  8.62s/it, 401.44/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 41%|████████▋            | 37/90 [06:48<07:17,  8.26s/it, 408.85/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 42%|████████▊            | 38/90 [06:56<07:06,  8.21s/it, 416.94/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 43%|█████████            | 39/90 [07:08<07:51,  9.24s/it, 428.58/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 44%|█████████▎           | 40/90 [07:15<07:09,  8.59s/it, 435.67/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 46%|█████████▌           | 41/90 [07:17<05:27,  6.68s/it, 437.88/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 47%|█████████▊           | 42/90 [07:20<04:24,  5.52s/it, 440.70/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 48%|██████████           | 43/90 [07:24<03:48,  4.86s/it, 444.01/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 49%|██████████▎          | 44/90 [07:29<03:55,  5.12s/it, 449.75/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 50%|██████████▌          | 45/90 [07:34<03:41,  4.93s/it, 454.22/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 51%|██████████▋          | 46/90 [07:36<02:55,  3.99s/it, 456.03/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 52%|██████████▉          | 47/90 [07:39<02:41,  3.75s/it, 459.22/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 53%|███████████▏         | 48/90 [07:50<04:08,  5.91s/it, 470.17/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 54%|███████████▍         | 49/90 [07:52<03:21,  4.91s/it, 472.76/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 56%|███████████▋         | 50/90 [07:55<02:51,  4.29s/it, 475.60/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 57%|███████████▉         | 51/90 [08:03<03:33,  5.49s/it, 483.87/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 58%|████████████▏        | 52/90 [08:05<02:47,  4.40s/it, 485.73/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 59%|████████████▎        | 53/90 [08:08<02:26,  3.97s/it, 488.69/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 60%|████████████▌        | 54/90 [08:21<03:58,  6.62s/it, 501.52/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 61%|████████████▊        | 55/90 [08:25<03:20,  5.72s/it, 505.11/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 62%|█████████████        | 56/90 [08:30<03:06,  5.48s/it, 510.03/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 63%|█████████████▎       | 57/90 [08:32<02:26,  4.43s/it, 512.01/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 64%|█████████████▌       | 58/90 [08:34<01:59,  3.74s/it, 514.14/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 66%|█████████████▊       | 59/90 [08:42<02:35,  5.03s/it, 522.17/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 67%|██████████████       | 60/90 [08:55<03:45,  7.50s/it, 535.46/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 68%|██████████████▏      | 61/90 [09:00<03:19,  6.89s/it, 540.91/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 69%|██████████████▍      | 62/90 [09:05<02:53,  6.18s/it, 545.45/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 70%|██████████████▋      | 63/90 [09:07<02:14,  4.99s/it, 547.67/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 71%|██████████████▉      | 64/90 [09:11<01:58,  4.54s/it, 551.16/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 72%|███████████████▏     | 65/90 [09:12<01:30,  3.62s/it, 552.63/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 73%|███████████████▍     | 66/90 [09:15<01:18,  3.26s/it, 555.04/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 74%|███████████████▋     | 67/90 [09:29<02:35,  6.74s/it, 569.92/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 76%|███████████████▊     | 68/90 [09:34<02:15,  6.16s/it, 574.70/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 77%|████████████████     | 69/90 [09:38<01:53,  5.40s/it, 578.35/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 78%|████████████████▎    | 70/90 [09:41<01:33,  4.68s/it, 581.34/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 79%|████████████████▌    | 71/90 [09:43<01:16,  4.01s/it, 583.77/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 80%|████████████████▊    | 72/90 [09:47<01:11,  3.97s/it, 587.65/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 81%|█████████████████    | 73/90 [09:56<01:31,  5.39s/it, 596.36/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 82%|█████████████████▎   | 74/90 [10:05<01:42,  6.42s/it, 605.19/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 83%|█████████████████▌   | 75/90 [10:08<01:23,  5.56s/it, 608.72/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 84%|█████████████████▋   | 76/90 [10:12<01:09,  4.96s/it, 612.29/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 86%|█████████████████▉   | 77/90 [10:28<01:49,  8.41s/it, 628.75/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 87%|██████████████████▏  | 78/90 [10:31<01:20,  6.69s/it, 631.43/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 88%|██████████████████▍  | 79/90 [10:33<00:58,  5.28s/it, 633.42/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 89%|██████████████████▋  | 80/90 [10:42<01:05,  6.57s/it, 642.99/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 90%|██████████████████▉  | 81/90 [10:47<00:53,  5.98s/it, 647.61/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 91%|███████████████████▏ | 82/90 [10:55<00:51,  6.43s/it, 655.09/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 92%|███████████████████▎ | 83/90 [10:57<00:37,  5.35s/it, 657.91/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 93%|███████████████████▌ | 84/90 [11:02<00:31,  5.19s/it, 662.74/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 94%|███████████████████▊ | 85/90 [11:05<00:22,  4.58s/it, 665.90/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 96%|████████████████████ | 86/90 [11:16<00:25,  6.41s/it, 676.57/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 97%|████████████████████▎| 87/90 [11:20<00:16,  5.55s/it, 680.11/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 98%|████████████████████▌| 88/90 [11:22<00:09,  4.51s/it, 682.19/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      " 99%|████████████████████▊| 89/90 [11:26<00:04,  4.30s/it, 686.01/20000 seconds]bn_index 1\n",
      "bn_index 4\n",
      "bn_index 8\n",
      "bn_index 11\n",
      "bn_index 15\n",
      "bn_index 18\n",
      "new model GraphModule(\n",
      "  (seq_blocks): Module(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    seq_blocks_0 = getattr(self.seq_blocks, \"0\")(x);  x = None\n",
      "    seq_blocks_1 = getattr(self.seq_blocks, \"1\")(seq_blocks_0);  seq_blocks_0 = None\n",
      "    seq_blocks_2 = getattr(self.seq_blocks, \"2\")(seq_blocks_1);  seq_blocks_1 = None\n",
      "    seq_blocks_3 = getattr(self.seq_blocks, \"3\")(seq_blocks_2);  seq_blocks_2 = None\n",
      "    seq_blocks_4 = getattr(self.seq_blocks, \"4\")(seq_blocks_3);  seq_blocks_3 = None\n",
      "    seq_blocks_5 = getattr(self.seq_blocks, \"5\")(seq_blocks_4);  seq_blocks_4 = None\n",
      "    seq_blocks_6 = getattr(self.seq_blocks, \"6\")(seq_blocks_5);  seq_blocks_5 = None\n",
      "    seq_blocks_7 = getattr(self.seq_blocks, \"7\")(seq_blocks_6);  seq_blocks_6 = None\n",
      "    seq_blocks_8 = getattr(self.seq_blocks, \"8\")(seq_blocks_7);  seq_blocks_7 = None\n",
      "    seq_blocks_9 = getattr(self.seq_blocks, \"9\")(seq_blocks_8);  seq_blocks_8 = None\n",
      "    seq_blocks_10 = getattr(self.seq_blocks, \"10\")(seq_blocks_9);  seq_blocks_9 = None\n",
      "    seq_blocks_11 = getattr(self.seq_blocks, \"11\")(seq_blocks_10);  seq_blocks_10 = None\n",
      "    seq_blocks_12 = getattr(self.seq_blocks, \"12\")(seq_blocks_11);  seq_blocks_11 = None\n",
      "    seq_blocks_13 = getattr(self.seq_blocks, \"13\")(seq_blocks_12);  seq_blocks_12 = None\n",
      "    seq_blocks_14 = getattr(self.seq_blocks, \"14\")(seq_blocks_13);  seq_blocks_13 = None\n",
      "    seq_blocks_15 = getattr(self.seq_blocks, \"15\")(seq_blocks_14);  seq_blocks_14 = None\n",
      "    seq_blocks_16 = getattr(self.seq_blocks, \"16\")(seq_blocks_15);  seq_blocks_15 = None\n",
      "    seq_blocks_17 = getattr(self.seq_blocks, \"17\")(seq_blocks_16);  seq_blocks_16 = None\n",
      "    seq_blocks_18 = getattr(self.seq_blocks, \"18\")(seq_blocks_17);  seq_blocks_17 = None\n",
      "    seq_blocks_19 = getattr(self.seq_blocks, \"19\")(seq_blocks_18);  seq_blocks_18 = None\n",
      "    seq_blocks_20 = getattr(self.seq_blocks, \"20\")(seq_blocks_19);  seq_blocks_19 = None\n",
      "    seq_blocks_21 = getattr(self.seq_blocks, \"21\")(seq_blocks_20);  seq_blocks_20 = None\n",
      "    seq_blocks_22 = getattr(self.seq_blocks, \"22\")(seq_blocks_21);  seq_blocks_21 = None\n",
      "    seq_blocks_23 = getattr(self.seq_blocks, \"23\")(seq_blocks_22);  seq_blocks_22 = None\n",
      "    seq_blocks_24 = getattr(self.seq_blocks, \"24\")(seq_blocks_23);  seq_blocks_23 = None\n",
      "    seq_blocks_25 = getattr(self.seq_blocks, \"25\")(seq_blocks_24);  seq_blocks_24 = None\n",
      "    seq_blocks_26 = getattr(self.seq_blocks, \"26\")(seq_blocks_25);  seq_blocks_25 = None\n",
      "    return seq_blocks_26\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "\u001b[33mWARNING \u001b[0m \u001b[34mNo quantized layers found in the model, set average_bitwidth to 32\u001b[0m\n",
      "100%|█████████████████████| 90/90 [11:32<00:00,  7.69s/it, 692.02/20000 seconds]\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mBest trial(s):\n",
      "Best trial(s):\n",
      "|    |   number | software_metrics                   | hardware_metrics                                | scaled_metrics                               |\n",
      "|----+----------+------------------------------------+-------------------------------------------------+----------------------------------------------|\n",
      "|  0 |        1 | {'loss': 2.306, 'accuracy': 0.117} | {'average_bitwidth': 32, 'memory_density': 1.0} | {'accuracy': 0.117, 'average_bitwidth': 6.4} |\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSearching is completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optional task\n",
    "# cifar10 + vgg7 + tpe\n",
    "\n",
    "!./ch search --config configs/examples/cifar10_vgg.toml --load ../mase_output/vgg7_classification_cifar10_2024-02-01/software/training_ckpts/best.ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
